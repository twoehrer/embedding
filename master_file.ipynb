{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined File\n",
    "\n",
    "This file should combine all the other experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from matplotlib.colors import to_rgb\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Training Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Params\n",
    "num_hidden = 6 # number of hidden layers. The total network has additionl 2 layers: input to hidden and hidden to output\n",
    "input_dim = 2\n",
    "hidden_dim = 2\n",
    "output_dim = 1\n",
    "activation = 'tanh' #'relu' and 'tanh' are supported\n",
    "\n",
    "# Training Params\n",
    "load_file = None\n",
    "cross_entropy = True #True supported with binary classification only\n",
    "num_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.training\n",
    "from models.training import make_circles_uniform\n",
    "\n",
    "# Generate training data\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = np.random.randint(1000)\n",
    "# seed = 163\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "#footnnote to display on plots to make sure that plots and model/trainign params do not get confused\n",
    "footnote = f'num_hidden={num_hidden}, hidden_dim={hidden_dim}, output_dim={output_dim}, act={activation}, seed={seed}, ce={cross_entropy}'\n",
    "\n",
    "n_points = 2000 #number of points in the dataset\n",
    "\n",
    "inner_radius = 0.5\n",
    "outer_radius = 1\n",
    "buffer = 0.2\n",
    "\n",
    "import importlib\n",
    "importlib.reload(models.training) # Reload the module\n",
    "\n",
    "train_loader, test_loader = make_circles_uniform(output_dim = 1, n_samples = n_points, inner_radius = 0.5, outer_radius = 1.0, buffer = 0.1, cross_entropy=cross_entropy, seed = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input, label in train_loader:\n",
    "    print(input[:5], label[:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model and training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to reload models.resnet module after changes without restarting the kernel\n",
    "# import importlib\n",
    "# import models.resnets\n",
    "# importlib.reload(models.resnets) # Reload the module\n",
    "from models.resnets import ResNet\n",
    "\n",
    "\n",
    "# class ResidualBlock(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Residual Block that includes a batch normalization layer and a skip connection with adjustable skip parameter. \n",
    "#     skip_param = 0 means no skip connection, skip_param = 1 means standard skip connection.\n",
    "#     The activation function can be set to 'relu', 'tanh', or 'id' (identity).\n",
    "#     \"\"\" \n",
    "    \n",
    "#     def __init__(self, features, skip_param = 1, activation = 'relu', batchnorm = True):\n",
    "\n",
    "#         super(ResidualBlock, self).__init__()\n",
    "#         self.fc = nn.Linear(features, features)\n",
    "#         if batchnorm: #batchnorm is important to stabilize the training for deeper networks\n",
    "#             self.bn = nn.BatchNorm1d(features)\n",
    "            \n",
    "#         if activation == 'relu':\n",
    "#             self.activation = nn.ReLU()\n",
    "#         if activation == 'tanh':\n",
    "#             self.activation = nn.Tanh()\n",
    "#         if activation == 'id':\n",
    "#             self.activation = nn.Identity()\n",
    "#         self.skip_param = skip_param\n",
    "#     def forward(self, x):\n",
    "#         identity = x #cont here\n",
    "#         out = self.fc(x)\n",
    "#         if hasattr(self, 'bn'):\n",
    "#             out = self.bn(out)\n",
    "#         out = self.activation(out)\n",
    "#         out = out + self.skip_param * identity\n",
    "#         return out\n",
    "\n",
    "# class ResNet(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim, num_hidden, skip_param = 1, activation = 'relu'):\n",
    "        \n",
    "#         super(ResNet, self).__init__()\n",
    "#         self.num_hidden = num_hidden\n",
    "#         self.input_fc = nn.Linear(input_dim, hidden_dim)\n",
    "#         if activation == 'relu':\n",
    "#             self.activation = nn.ReLU()\n",
    "#         if activation == 'tanh':\n",
    "#             self.activation = nn.Tanh()\n",
    "#         if activation == 'id':\n",
    "#             self.activation = nn.Identity()\n",
    "            \n",
    "#         self.res_blocks = nn.Sequential(\n",
    "#             *[ResidualBlock(hidden_dim, skip_param=skip_param, activation = activation) for _ in range(num_hidden)]\n",
    "#         )\n",
    "#         self.output_fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "#     def forward(self, x, output_layer = True):\n",
    "#         x = self.activation(self.input_fc(x))\n",
    "#         x = self.res_blocks(x)\n",
    "#         if output_layer:\n",
    "#             x = self.output_fc(x)\n",
    "#             x = torch.sigmoid(x)\n",
    "#         return x\n",
    "    \n",
    "#     '''\n",
    "#     sub_model is used to access a partial network of the input to output network\n",
    "#     layers are counted from 0 (input to hidden dim) until nth layer ( (n-1)th hidden layer to output layer)\n",
    "#     from_layer is the starting layer that is included in the sub_model\n",
    "#     to_layer is the final layer included in the sub_model\n",
    "#     e.g. from_layer = 1, to_layer = 2 includes the first two ResBlocks\n",
    "#     initial layer is counted as layer 0\n",
    "#     hidden to output layer is counted as final layer\n",
    "#     '''\n",
    "#     def sub_model(self, x, from_layer, to_layer):\n",
    "#         if to_layer > self.num_hidden + 1:\n",
    "#             print('Error: to_layer is larger than existing number of layers')\n",
    "#             return\n",
    "#         if from_layer > to_layer:\n",
    "#             print('Error: to_layer cannot be larger than from_layer')\n",
    "        \n",
    "        \n",
    "#         if from_layer == 0:\n",
    "#             x = self.activation(self.input_fc(x))\n",
    "#             from_layer += 1 #if from_layer = 0 I need to increase the from_layer count\n",
    "\n",
    "#         if to_layer > 0 and from_layer < self.num_hidden + 1:\n",
    "#             reduced_block = self.res_blocks[from_layer - 1 : to_layer] #from layer 1 to 2 means hidden layer 0 to hidden layer 1\n",
    "#             x = reduced_block(x)\n",
    "#         if to_layer == self.num_hidden + 1:\n",
    "#             x = self.output_fc(x)\n",
    "#         return x\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.training import compute_accuracy, train_model, train_until_threshold, plot_loss_curve\n",
    "\n",
    "# def compute_accuracy(y_pred, y_true):\n",
    "#     \"\"\"\n",
    "#     computes accuracy of predictions against ground truth labels.\n",
    "#     only works for 1-dim output currently\n",
    "#     y_pred: float32 predictions (sigmoid outputs)\n",
    "#     y_true: float32 ground truth labels (0 or 1)\n",
    "#     \"\"\"\n",
    "#     y_pred_binary = (y_pred >= 0.5).int()\n",
    "#     y_true_binary = y_true.int()\n",
    "#     correct = (y_pred_binary == y_true_binary).sum().item()\n",
    "#     total = y_true.shape[0]\n",
    "#     return correct / total\n",
    "\n",
    "# import copy\n",
    "\n",
    "# def train_model(model, train_loader, test_loader,\n",
    "#                                 load_file = None, epochs=300, lr=0.01, patience=300, cross_entropy=True):\n",
    "#     \"\"\"\n",
    "#     Trains the model on the provided training data and evaluates it on the test data.\n",
    "#     patience is the number of epochs to wait for improvement before stopping training.\n",
    "#     If load_file is provided, it will load the model state from the specified file instead of training.\n",
    "#     Returns the trained model, best accuracy, and training losses.\n",
    "#     \"\"\"\n",
    "#     if load_file is None:  # Only enter retry loop if no model is being loaded\n",
    "    \n",
    "#         model.train()\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#         if cross_entropy:\n",
    "#             criterion = nn.BCELoss()\n",
    "#         else: criterion = nn.MSELoss()\n",
    "\n",
    "#         best_acc = 0\n",
    "#         patience_counter = 0\n",
    "#         losses = []\n",
    "\n",
    "\n",
    "\n",
    "#         for epoch in range(epochs):\n",
    "#             epoch_loss = 0\n",
    "#             for batch_X, batch_y in train_loader:\n",
    "#                 y_pred = model(batch_X)\n",
    "#                 loss = criterion(y_pred, batch_y)\n",
    "#                 epoch_loss += loss.item()\n",
    "\n",
    "#                 optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#             losses.append(epoch_loss / len(train_loader))\n",
    "\n",
    "#             # Evaluate on test data\n",
    "#             model.eval()\n",
    "#             with torch.no_grad():\n",
    "#                 acc_summed = 0.\n",
    "#                 counter = 0\n",
    "#                 for X_test, y_test in test_loader:\n",
    "#                     counter += 1\n",
    "#                     test_preds = model(X_test)\n",
    "#                     acc_summed += compute_accuracy(test_preds, y_test)\n",
    "#                 acc = acc_summed / counter\n",
    "#             model.train()\n",
    "\n",
    "#             if acc > best_acc:\n",
    "#                 best_acc = acc\n",
    "#                 best_model_state = copy.deepcopy(model.state_dict())\n",
    "#                 patience_counter = 0\n",
    "#             else:\n",
    "#                 patience_counter += 1\n",
    "#                 if patience_counter >= patience:\n",
    "#                     print(f\"â¹ï¸ Early stopping at epoch {epoch}, best acc: {best_acc:.3f}\")\n",
    "#                     break\n",
    "\n",
    "#             # At end, load the best model\n",
    "#         if patience_counter > 0:\n",
    "#             model.load_state_dict(best_model_state)\n",
    "            \n",
    "#         # --- Save Checkpoint ---\n",
    "#         checkpoint = {\n",
    "#         'model_state_dict': model.state_dict(),\n",
    "#         'optimizer_state_dict': optimizer.state_dict(), # Good practice to save optimizer state too\n",
    "#         'losses': losses,\n",
    "#         'seed': seed,\n",
    "#         'epoch': epoch, # Save the last epoch number\n",
    "#         'input_dim': input_dim, # Save hyperparameters for verification/reproducibility\n",
    "#         'hidden_dim': hidden_dim,\n",
    "#         'output_dim': output_dim,\n",
    "#         'num_blocks': num_hidden,\n",
    "#         'cross_entropy': cross_entropy,\n",
    "#         'accuracy': best_acc,\n",
    "#         'activation': model.activation,\n",
    "#         'footnote': footnote  # Save the footnote for plots\n",
    "#         }\n",
    "#         save_path = f'last.pth'\n",
    "#         torch.save(checkpoint, save_path)\n",
    "#         print(f'Checkpoint saved to {save_path}')\n",
    "#         # We have the losses from training directly\n",
    "\n",
    "#         return model, best_acc, losses  # <--- return the best model!\n",
    "    \n",
    "    \n",
    "#     else: # If loading a model, skip training and just load the model state\n",
    "        \n",
    "#         load_path = load_file + '.pth'\n",
    "#     try:\n",
    "#         print(f\"--- Loading Checkpoint from: {load_path} ---\")\n",
    "#         checkpoint = torch.load(load_path)\n",
    "\n",
    "#         # Load model state\n",
    "#         model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "#         # Load losses and seed from the checkpoint\n",
    "#         losses = checkpoint.get('losses', []) # Use .get for backward compatibility if 'losses' key is missing\n",
    "#         loaded_seed = checkpoint.get('seed', 'Not Found') # Use .get for backward compatibility\n",
    "\n",
    "#         # Optionally load optimizer state if you plan to resume training\n",
    "#         # optimizer = torch.optim.Adam(model.parameters()) # Re-initialize optimizer\n",
    "#         # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#         # start_epoch = checkpoint['epoch'] + 1 # To resume training\n",
    "\n",
    "#         # Load other saved info (optional, but good for verification)\n",
    "#         loaded_input_dim = checkpoint.get('input_dim', 'Not Found')\n",
    "#         loaded_hidden_dim = checkpoint.get('hidden_dim', 'Not Found')\n",
    "#         loaded_output_dim = checkpoint.get('output_dim', 'Not Found')\n",
    "#         loaded_num_blocks = checkpoint.get('num_blocks', 'Not Found')\n",
    "#         loaded_cross_entropy = checkpoint.get('cross_entropy', 'Not Found')\n",
    "#         last_epoch = checkpoint.get('epoch', 'Not Found')\n",
    "#         best_acc = checkpoint.get('accuracy', 0.0)  # Load the best accuracy\n",
    "#         activation = checkpoint.get('activation', 'Not Found')\n",
    "    \n",
    "\n",
    "\n",
    "#         print(f\"Model state loaded successfully.\")\n",
    "#         print(f\"Loaded training losses (Length: {len(losses)}).\")\n",
    "#         print(f\"Original training seed: {loaded_seed}\")\n",
    "#         print(f\"Model trained for {last_epoch + 1 if isinstance(last_epoch, int) else 'N/A'} epochs.\")\n",
    "#         print(f\"Saved Hyperparameters: Input={loaded_input_dim}, Hidden={loaded_hidden_dim}, Output={loaded_output_dim}, Blocks={loaded_num_blocks}, CrossEntropy={loaded_cross_entropy}\")\n",
    "\n",
    "\n",
    "#         model.eval() # Set model to evaluation mode after loading\n",
    "#         print(\"Model set to evaluation mode.\")\n",
    "        \n",
    "#         return model, best_acc, losses\n",
    "\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"Error: Checkpoint file not found at {load_path}\")\n",
    "#         losses = [] # Ensure losses is an empty list if loading failed\n",
    "\n",
    "\n",
    "# def train_until_threshold(model_class, train_loader, test_loader, \n",
    "#                           load_file = None, cross_entropy=True,max_retries=10, threshold=0.95, **model_kwargs):\n",
    "#     if load_file is None:\n",
    "#         for attempt in range(1, max_retries + 1):\n",
    "#             seed = np.random.randint(1000)\n",
    "#             np.random.seed(seed)\n",
    "#             torch.manual_seed(seed)\n",
    "#             model = model_class(**model_kwargs)\n",
    "#             model, acc, losses = train_model(model, train_loader, test_loader, cross_entropy=cross_entropy)\n",
    "#             print(f\"[Attempt {attempt}] Accuracy: {acc:.3f}\")\n",
    "#             if acc >= threshold:\n",
    "#                 print(f\"âœ… Success after {attempt} attempt(s)!\")\n",
    "#                 return model, acc, losses\n",
    "#         print(\"âŒ Failed to reach threshold.\")\n",
    "#         return model, acc, losses\n",
    "#     else:\n",
    "#         print(\"Loading model, skipping training.\")\n",
    "#         model = model_class(**model_kwargs)\n",
    "#         model, acc, losses = train_model(model, train_loader, test_loader, load_file=load_file, cross_entropy=cross_entropy)\n",
    "#         return model, acc, losses\n",
    "\n",
    "\n",
    "# def plot_loss_curve(losses, title=\"Training Loss\", filename = None):\n",
    "#     plt.figure(figsize=(6, 4))\n",
    "#     plt.plot(losses, label=\"Loss\")\n",
    "#     plt.xlabel(\"Epoch\")\n",
    "#     plt.ylabel(\"Binary Cross Entropy Loss\")\n",
    "#     plt.title(title)\n",
    "#     plt.grid(True)\n",
    "#     plt.legend()\n",
    "#     plt.tight_layout()\n",
    "#     if filename is not None:\n",
    "#         plt.savefig(filename + '.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "#     plt.show()\n",
    "    \n",
    "# from matplotlib.colors import to_rgb, LinearSegmentedColormap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant width 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = ResNet(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_hidden=num_hidden, skip_param=1, activation=activation)\n",
    "\n",
    "# Train models\n",
    "model_base, acc_base, losses_base = train_until_threshold(ResNet,\n",
    "    train_loader, test_loader,\n",
    "    load_file = load_file, max_retries=5, threshold=0.95,\n",
    "    input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_hidden=num_hidden, skip_param=0, activation=activation\n",
    ")\n",
    "\n",
    "plot_loss_curve(losses_base, title=f\"Base Model Loss Curve\", filename = 'ff6hidden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import plots.plots \n",
    "from plots.plots import plot_decision_boundary, plot_level_sets\n",
    "importlib.reload(plots.plots) # Reload the module\n",
    "\n",
    "X_test, y_test = next(iter(test_loader))\n",
    "plot_decision_boundary(model_base, X_test, y_test, show=True, file_name= 'ff6hiddencirc' + str(num_hidden), footnote = footnote, amount_levels= 100)\n",
    "plot_level_sets(model_base, show=True, file_name= 'ff6hiddencirc_contour' + str(num_hidden), footnote = footnote, amount_levels= 20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmented model: width 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 3\n",
    "num_hidden = 1\n",
    "\n",
    "model_aug, acc_aug, losses_aug = train_until_threshold(ResNet,\n",
    "    train_loader, test_loader,\n",
    "    load_file = load_file, max_retries=5, threshold=0.95,\n",
    "    input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_hidden=num_hidden, skip_param=0, activation=activation\n",
    ")\n",
    "\n",
    "plot_loss_curve(losses_aug, title=f\"Augmented Model Loss Curve\", filename = 'ff6hidden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = next(iter(test_loader))\n",
    "plot_decision_boundary(model_aug, X_test, y_test, show=True, file_name= 'ff6hiddencirc' + str(num_hidden), footnote = footnote, amount_levels= 100)\n",
    "plot_level_sets(model_aug, show=True, file_name= 'ff6hiddencirc_contour' + str(num_hidden), footnote = footnote, amount_levels= 20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_param = 1 # this sets model from feed forward to residual network\n",
    "\n",
    "num_hidden = 6\n",
    "hidden_dim = 2\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "model_res, acc_res, losses_res = train_until_threshold(ResNet,\n",
    "    train_loader, test_loader,\n",
    "    load_file = load_file, max_retries=5, threshold=0.95,\n",
    "    input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_hidden=num_hidden, skip_param=skip_param, activation=activation\n",
    ")\n",
    "\n",
    "plot_loss_curve(losses_res, title=f\"ResNet Model Loss Curve\", filename = 'ff6_res')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = next(iter(test_loader))\n",
    "plot_decision_boundary(model_res, X_test, y_test, show=True, file_name= 'ff6resnetcirc' + str(num_hidden), footnote = footnote, amount_levels= 100)\n",
    "plot_level_sets(model_res, show=True, file_name= 'ff6resnetcirc' + str(num_hidden), footnote = footnote, amount_levels= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test, y_test = next(iter(test_loader))\n",
    "num_epochs = 300\n",
    "\n",
    "skip_values = np.linspace(0, 5, 6)  # e.g., [0.0, 0.125, ..., 1.0]\n",
    "# skip_values = [0, 0, 0 , 0 , 0]  # e.g., [0.0, 0.125, ..., 1.0]\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(skip_values) / n_cols))\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(3.5 * n_cols, 4 * n_rows), facecolor='white')\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.) \n",
    "# num_epochs = 10\n",
    "for idx, skip in enumerate(skip_values):\n",
    "    print(f\"Training model with skip_param = {skip:.2f}\")\n",
    "    \n",
    "    # seed = 163\n",
    "    print(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    model = ResNet(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim,\n",
    "                   num_hidden=num_hidden, skip_param=skip, activation=activation)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.BCEWithLogitsLoss() if cross_entropy else nn.MSELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    ax = axes.flatten()[idx]\n",
    "    plot_decision_boundary(model, X_test, y_test, title=f\"Skip: {skip:.2f}\", ax=ax, show=False, colorbar=False, show_points=False, amount_levels=100)\n",
    "    \n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    ax.axis('tight')\n",
    "    ax.set_aspect('equal') \n",
    "    \n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(len(skip_values), len(axes.flatten())):\n",
    "    fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "fig.suptitle(f\"ResNets with amount layers = {num_hidden + 2} and different weights of shortcut\", fontsize=16)\n",
    "\n",
    "# ðŸ’¡ Leave room at top for the title\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "plt.savefig('ff6hidden_skip_params.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(model, X_test, y_test, show=True, file_name= 'test1' + str(num_hidden), footnote = footnote, amount_levels= 100)\n",
    "\n",
    "plot_decision_boundary(model, X_test, y_test, show=True, file_name= 'test1' + str(num_hidden), footnote = footnote, amount_levels= 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular value computations and plotting\n",
    "We want to determine singular points in the compact space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a grid over the input space.\n",
    "grid_size = 200 # Adjust as needed.\n",
    "\n",
    "def psi_manual(x, func):\n",
    "    \"\"\"\n",
    "    x: a tensor of shape (2,) representing a point in R^2.\n",
    "    model: a function mapping R^2 to R^output_dim.\n",
    "    \n",
    "    Returns:\n",
    "      The smallest singular value of the Jacobian of model at x.\n",
    "    \"\"\"\n",
    "    # Ensure x is a leaf variable with gradient tracking enabled.\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # Define a lambda function to ensure accurate input-output mapping\n",
    "    # func = lambda inp: model(inp, output_layer = False)\n",
    "    \n",
    "    # Compute the Jacobian using torch.autograd.functional.jacobian (compatible with Python 3.8)\n",
    "    jacobian = torch.autograd.functional.jacobian(func, x, create_graph=True)\n",
    "    \n",
    "    # Compute singular values using svdvals (available in PyTorch 1.8, compatible with Python 3.8)\n",
    "    singular_values = torch.svd(jacobian, compute_uv=False)[1] #svd interprets here the jacobian as a SQUARE matrix of the largest dimension, hence it \n",
    "    \n",
    "   \n",
    "    return singular_values.detach().numpy()\n",
    "\n",
    "\n",
    "\n",
    "x_range = np.linspace(-1, 1, grid_size)\n",
    "y_range = np.linspace(-1, 1, grid_size)\n",
    "psi_values = np.zeros((grid_size, grid_size, 2))\n",
    "\n",
    "# Put the model in evaluation mode.\n",
    "model.eval()\n",
    "func = lambda inp: model(inp, output_layer = False)\n",
    "\n",
    "# Evaluate psi(x) over the grid.\n",
    "for i, xv in enumerate(x_range):\n",
    "    for j, yv in enumerate(y_range):\n",
    "        # Create a 2D point as a torch tensor.\n",
    "        x_point = torch.tensor([xv, yv], dtype=torch.float32)\n",
    "        psi_values[j, i,:] = psi_manual(x_point, func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the contour corresponding to psi ~ 0.\n",
    "plt.figure(figsize=(8, 6))\n",
    "# Here we plot the contour at a small level, e.g., 0.01.\n",
    "# CS = plt.contour(x_range, y_range, psi_values, levels=[0,0.05,0.1,0.2,0.3], colors='red')\n",
    "\n",
    "# Define the number of levels for the contour plot\n",
    "vmin1, vmax1 = psi_values[:, :, 1].min(), psi_values[:, :, 1].max()\n",
    "num_levels = 100\n",
    "levels1 = np.linspace(0, vmax1, num_levels)\n",
    "# Create the contour plot using the 'binary' colormap\n",
    "CS = plt.contourf(x_range, y_range, psi_values[:,:,1], levels=levels1, cmap = 'viridis')\n",
    "cbar = plt.colorbar(CS)\n",
    "plt.title('smallest singular value of Jacobian \\nno output layer')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()\n",
    "\n",
    "# Plot the contour corresponding to psi ~ 0.\n",
    "plt.figure(figsize=(8, 6))\n",
    "# Here we plot the contour at a small level, e.g., 0.01.\n",
    "# CS = plt.contour(x_range, y_range, psi_values, levels=[0,0.05,0.1,0.2,0.3], colors='red')\n",
    "\n",
    "# Define the number of levels for the contour plot\n",
    "\n",
    "vmin0, vmax0 = psi_values[:, :, 0].min(), psi_values[:, :, 0].max()\n",
    "num_levels = 100\n",
    "levels0 = np.linspace(0, vmax0, num_levels)\n",
    "# Create the contour plot using the 'binary' colormap\n",
    "CS = plt.contourf(x_range, y_range, psi_values[:,:,0], levels=levels0, cmap = 'viridis')\n",
    "cbar = plt.colorbar(CS)\n",
    "plt.title('largest singular value of Jacobian \\nno output layer')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Put the model in evaluation mode.\n",
    "model.eval()\n",
    "\n",
    "# Prepare figure and axes\n",
    "fig, axes = plt.subplots(2, num_hidden + 2, figsize=(5 * (num_hidden + 2), 10))  # Adjust figsize if needed\n",
    "\n",
    "for layer in range(num_hidden + 2):\n",
    "    func = lambda inp: model.sub_model(inp, from_layer=0, to_layer = layer)\n",
    "\n",
    "    # Evaluate psi(x) over the grid.\n",
    "    for i, xv in enumerate(x_range):\n",
    "        for j, yv in enumerate(y_range):\n",
    "            x_point = torch.tensor([xv, yv], dtype=torch.float32)\n",
    "            psi_values[j, i, :] = psi_manual(x_point, func)\n",
    "\n",
    "    # Determine contour levels\n",
    "    vmin1, vmax1 = psi_values[:, :, 1].min(), psi_values[:, :, 1].max()\n",
    "    num_levels_contour = 200\n",
    "    levels1 = np.linspace(0, vmax1, num_levels_contour)\n",
    "\n",
    "    # Plot smallest singular value (index 1) - first row\n",
    "    ax = axes[0, layer] if num_hidden > 1 else axes[0]\n",
    "    cs = ax.contourf(x_range, y_range, psi_values[:, :, 1], levels=levels1, cmap='viridis')\n",
    "    fig.colorbar(cs, ax=ax)\n",
    "    ax.set_title(f'Min SV\\n layer_in = 0, layer_out = {layer}')\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    # Determine contour levels\n",
    "    vmin0, vmax0 = psi_values[:, :, 0].min(), psi_values[:, :, 0].max()\n",
    "    num_levels_contour = 200\n",
    "    levels0 = np.linspace(0, vmax0, num_levels_contour)\n",
    "    \n",
    "\n",
    "    # Plot largest singular value (index 0) - second row\n",
    "    ax = axes[1, layer] if num_hidden > 1 else axes[1]\n",
    "    cs = ax.contourf(x_range, y_range, psi_values[:, :, 0], levels=levels0, cmap='viridis')\n",
    "    fig.colorbar(cs, ax=ax)\n",
    "    ax.set_title(f'Max SV\\n layer_in = 0, layer_out = {layer}')\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Put the model in evaluation mode.\n",
    "model.eval()\n",
    "\n",
    "# Prepare figure and axes\n",
    "fig, axes = plt.subplots(2, num_hidden + 2, figsize=(5 * (num_hidden + 2), 10))  # Adjust figsize if needed\n",
    "\n",
    "x_range = np.linspace(-1, 1, grid_size)\n",
    "y_range = np.linspace(-1, 1, grid_size)\n",
    "\n",
    "for layer in range(num_hidden + 2):\n",
    "    func = lambda inp: model.sub_model(inp, from_layer = layer, to_layer=layer)\n",
    "\n",
    "    # Evaluate psi(x) over the grid.\n",
    "    for i, xv in enumerate(x_range):\n",
    "        for j, yv in enumerate(y_range):\n",
    "            x_point = torch.tensor([xv, yv], dtype=torch.float32)\n",
    "            psi_values[j, i, :] = psi_manual(x_point, func)\n",
    "\n",
    "    # Determine contour levels\n",
    "    vmin1, vmax1 = psi_values[:, :, 1].min(), psi_values[:, :, 1].max()\n",
    "    num_levels_contour = 200\n",
    "    levels1 = np.linspace(0, vmax1, num_levels_contour)\n",
    "\n",
    "    # Plot smallest singular value (index 1) - first row\n",
    "    ax = axes[0, layer] if num_hidden > 1 else axes[0]\n",
    "    cs = ax.contourf(x_range, y_range, psi_values[:, :, 1], \n",
    "                     levels=levels1,\n",
    "                     cmap='viridis')\n",
    "    fig.colorbar(cs, ax=ax)\n",
    "    ax.set_title(f'Min SV\\n layer {layer}')\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "\n",
    "    # Determine contour levels\n",
    "    vmin0, vmax0 = psi_values[:, :, 0].min(), psi_values[:, :, 0].max()\n",
    "    num_levels_contour = 200\n",
    "    levels0 = np.linspace(0, vmax0, num_levels_contour)\n",
    "\n",
    "    # Plot largest singular value (index 0) - second row\n",
    "    ax = axes[1, layer] if num_hidden > 1 else axes[1]\n",
    "    cs = ax.contourf(x_range, y_range, psi_values[:, :, 0],\n",
    "                     levels=levels0, \n",
    "                     cmap='viridis')\n",
    "    fig.colorbar(cs, ax=ax)\n",
    "    ax.set_title(f'Max SV\\n layer {layer}')\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare figure and axes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10 , 5))  # Adjust figsize if needed\n",
    "\n",
    "\n",
    "\n",
    "class TestFunc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestFunc, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.norm(x)**2\n",
    "    \n",
    "    \n",
    "def psi_manual_test(x, func):\n",
    "    \"\"\"\n",
    "    x: a tensor of shape (2,) representing a point in R^2.\n",
    "    model: a function mapping R^2 to R^output_dim.\n",
    "    \n",
    "    Returns:\n",
    "      The smallest singular value of the Jacobian of model at x.\n",
    "    \"\"\"\n",
    "    # Ensure x is a leaf variable with gradient tracking enabled.\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # Define a lambda function to ensure accurate input-output mapping\n",
    "    # func = lambda inp: model(inp, output_layer = False)\n",
    "    \n",
    "    # Compute the Jacobian using torch.autograd.functional.jacobian (compatible with Python 3.8)\n",
    "    jacobian = torch.autograd.functional.jacobian(func, x, create_graph=True).unsqueeze(0)\n",
    "    print(f'{jacobian = }')\n",
    "    \n",
    "    # Compute singular values using svdvals (available in PyTorch 1.8, compatible with Python 3.8)\n",
    "    singular_values = torch.svd(jacobian, compute_uv=False)[1]\n",
    "    print(f'{singular_values = }')\n",
    "    \n",
    "   \n",
    "    return singular_values.detach().numpy()\n",
    "\n",
    "test_func = TestFunc()\n",
    "\n",
    "# Evaluate psi(x) over the grid.\n",
    "for i, xv in enumerate(x_range):\n",
    "    for j, yv in enumerate(y_range):\n",
    "        x_point = torch.tensor([xv, yv], dtype=torch.float32)\n",
    "        psi_values[j, i, :] = psi_manual_test(x_point, test_func)\n",
    "        \n",
    "\n",
    "# Determine contour levels\n",
    "vmin1, vmax1 = psi_values[:, :, 1].min(), psi_values[:, :, 1].max()\n",
    "num_levels_contour = 200\n",
    "levels1 = np.linspace(0, vmax1, num_levels_contour)\n",
    "\n",
    "# Plot smallest singular value (index 1) - first row\n",
    "ax = axes[0]\n",
    "cs = ax.contourf(x_range, y_range, psi_values[:, :, 1], \n",
    "                    levels=levels1,\n",
    "                    cmap='viridis')\n",
    "fig.colorbar(cs, ax=ax)\n",
    "ax.set_title(f'Min SV\\n layer {layer}')\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "\n",
    "# Determine contour levels\n",
    "vmin0, vmax0 = psi_values[:, :, 0].min(), psi_values[:, :, 0].max()\n",
    "num_levels_contour = 200\n",
    "levels0 = np.linspace(0, vmax0, num_levels_contour)\n",
    "\n",
    "# Plot largest singular value (index 0) - second row\n",
    "ax = axes[1] \n",
    "cs = ax.contourf(x_range, y_range, psi_values[:, :, 0],\n",
    "                    levels=levels0, \n",
    "                    cmap='viridis')\n",
    "fig.colorbar(cs, ax=ax)\n",
    "ax.set_title(f'Max SV\\n layer {layer}')\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralODE",
   "language": "python",
   "name": "neuralode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
