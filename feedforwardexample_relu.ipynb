{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding restrictions for simple feed forward networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "[Kuehn, Kuntz] show that feed forward neural networks have embedding restrictions.\n",
    "We implement simple examples.\n",
    "\n",
    "**Example**: A classification of a classical inner and outer circle data set\n",
    "- The shape of the data set encourages to learn a generalization function similar to $|x|^2$ as a continuous function from $\\R^2 \\to \\R^1$.\n",
    "- To achieve high accuracy, the NN should be topologically equivalent to $|x|^2$.\n",
    "- But this would generate a minimum inside the decision boundary\n",
    "\n",
    "**Outcome**:\n",
    "We will observe that certain architectures fail to generate high accuracy despite a large amount of parameters due to the embedding restrictions\n",
    "\n",
    "**Architectures**:\n",
    "1. (Fully connected) feed forward NNs (FFNN) of 2d input with constant width 2 and 1d output\n",
    "2. FFNNs with bottle neck: 1 layer of width 1 inbetween two layers of width 2\n",
    "3. Shallow FFNNs with width 3\n",
    "4. Note: ResNet architectures and neuralODEs will be investigated in separate file\n",
    "\n",
    "\n",
    "It holds that FFNN of constant input dimension width and with full rank parameter matrices are not able to generate singular points $x^*$, i.e. a Jacobian w.r.t. the inputs that is the 0 matrix at $x^*$. As such, any NN of two dimensional input with layers of width two cannot generate an input to output function that is topologically equivalent to $|x|^2$.\n",
    "\n",
    "It further holds that even a bottle neck, that is a sequence of 3 layers where the middle layer has a strictly smaller dimension, cannot generate a singular point either. As a result no FFNN with at maximum 2 neurons per layer can approximate $|x|^2$.\n",
    "\n",
    "There will be\n",
    "\n",
    "\n",
    "CONTINUE HERE BY ADDING THE TRAINING DATA FROM THE OTHER FILE TO THIS FILE SO IT WORKS. I Already added the test_dataloader to the other file which was not used before there. but here i want to use it for accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Imports and data prep\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from models.training import make_circles_uniform\n",
    "\n",
    "# seed = 43\n",
    "seed = np.random.randint(1000)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "depth = 15 # Number of layers\n",
    "cross_entropy = True\n",
    "batch_size=128\n",
    "\n",
    "\n",
    "train_loader, test_loader = make_circles_uniform(output_dim = 1., n_samples = 2000, inner_radius = 0.5, outer_radius = 1.0, buffer = 0.3, cross_entropy=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the feed forward neural network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFnet(nn.Module):\n",
    "    def __init__(self, depth=depth, width=2, bottleneck=False, activation='tanh'):\n",
    "        super().__init__()\n",
    "        act_fn = nn.Tanh if activation == 'tanh' else nn.ReLU\n",
    "        self.activations = []\n",
    "        layers = []\n",
    "        in_features = 2\n",
    "\n",
    "        for i in range(depth):\n",
    "            out_features = 1 if bottleneck and i == depth - 2 else width\n",
    "            layers.append(nn.Linear(in_features, out_features))\n",
    "            layers.append(nn.BatchNorm1d(out_features))\n",
    "            layers.append(act_fn())\n",
    "            in_features = out_features\n",
    "\n",
    "        layers.append(nn.Linear(in_features, 1))  # output layer\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, collect_activations=False):\n",
    "        # Add a batch dimension if input is 1D\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)  # Shape becomes [1, features]\n",
    "        self.activations = []\n",
    "        for layer in self.net[:-1]:\n",
    "            x = layer(x)\n",
    "            if collect_activations and isinstance(layer, (nn.Tanh, nn.ReLU)):\n",
    "                self.activations.append(x.detach().cpu().numpy())\n",
    "        x = self.net[-1](x)\n",
    "        return torch.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    computes accuracy of predictions against ground truth labels.\n",
    "    only works for 1-dim output currently\n",
    "    y_pred: float32 predictions (sigmoid outputs)\n",
    "    y_true: float32 ground truth labels (0 or 1)\n",
    "    \"\"\"\n",
    "    y_pred_binary = (y_pred >= 0.5).int()\n",
    "    y_true_binary = y_true.int()\n",
    "    correct = (y_pred_binary == y_true_binary).sum().item()\n",
    "    total = y_true.shape[0]\n",
    "    return correct / total\n",
    "\n",
    "import copy\n",
    "\n",
    "def train_model(model, train_loader, test_loader,\n",
    "                                epochs=300, lr=0.01, patience=300, batch_size=128):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "    best_acc = 0\n",
    "    patience_counter = 0\n",
    "    losses = []\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            y_pred = model(batch_X)\n",
    "            loss = criterion(y_pred, batch_y)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        losses.append(epoch_loss / len(train_loader))\n",
    "\n",
    "        # Evaluate on test data\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            acc_summed = 0.\n",
    "            counter = 0\n",
    "            for X_test, y_test in test_loader:\n",
    "                counter += 1\n",
    "                test_preds = model(X_test)\n",
    "                acc_summed += compute_accuracy(test_preds, y_test)\n",
    "            acc = acc_summed / counter\n",
    "        model.train()\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"‚èπÔ∏è Early stopping at epoch {epoch}, best acc: {best_acc:.3f}\")\n",
    "                break\n",
    "\n",
    "        # At end, load the best model\n",
    "    if patience_counter > 0:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model, best_acc, losses  # <--- return the best model!\n",
    "\n",
    "\n",
    "\n",
    "def train_until_threshold(model_class, train_loader, test_loader,\n",
    "                          max_retries=10, threshold=0.95, **model_kwargs):\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        model = model_class(**model_kwargs)\n",
    "        model, acc, losses = train_model(model, train_loader, test_loader)\n",
    "        print(f\"[Attempt {attempt}] Accuracy: {acc:.3f}\")\n",
    "        if acc >= threshold:\n",
    "            print(f\"‚úÖ Success after {attempt} attempt(s)!\")\n",
    "            return model, acc, losses\n",
    "    print(\"‚ùå Failed to reach threshold.\")\n",
    "    return model, acc, losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(losses, title=\"Training Loss\"):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(losses, label=\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Binary Cross Entropy Loss\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "from matplotlib.colors import to_rgb, LinearSegmentedColormap\n",
    "\n",
    "\n",
    "def plot_decision_boundary(model, X, y, title=\"Decision Boundary\", margin = 0.2):\n",
    "    \n",
    "    colors = [to_rgb(\"C0\"), [1, 1, 1], to_rgb(\"C1\")] # first color is orange, last is blue\n",
    "    cm = LinearSegmentedColormap.from_list(\n",
    "                \"Custom\", colors, N=40)\n",
    "    \n",
    "    model.eval()\n",
    "   \n",
    "    x_min, x_max = X[:, 0].min() - margin, X[:, 0].max() + margin\n",
    "    y_min, y_max = X[:, 1].min() - margin, X[:, 1].max() + margin\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    grid_tensor = torch.tensor(grid, dtype=torch.float32)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        preds = model(grid_tensor).numpy().reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, preds, levels=50, cmap=cm, alpha=0.8)\n",
    "    # plt.scatter(X[:, 0], X[:, 1], c=y.squeeze(), cmap='bwr', edgecolors='k')\n",
    "    plt.colorbar(label='Prediction Probability')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=25, c = y.squeeze(), cmap = cm, edgecolors='black', linewidths=0.5, alpha=0.9)\n",
    "    # plt.scatter(inside_points[:500, 0], inside_points[:500, 1], s=25, c='C0',  edgecolors='black', linewidths=0.5, alpha=0.5,  label='Inside Points')\n",
    "    plt.title(title)\n",
    "    # plt.axis('equal')\n",
    "    plt.grid(False)\n",
    "    plt.xlim(-1, 1)\n",
    "    plt.ylim(-1, 1)\n",
    "    plt.axis('tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # model.eval()\n",
    "    # x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    # y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    # xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "    #                      np.linspace(y_min, y_max, 200))\n",
    "    # grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    # grid_tensor = torch.tensor(grid, dtype=torch.float32)\n",
    "    # with torch.no_grad():\n",
    "    #     preds = model(grid_tensor).numpy().reshape(xx.shape)\n",
    "    # plt.figure(figsize=(7, 6))\n",
    "    # plt.contourf(xx, yy, preds, levels=50, cmap='coolwarm')\n",
    "    # plt.scatter(X[:, 0], X[:, 1], c=y.squeeze(), cmap='bwr', edgecolors='k')\n",
    "    # plt.title(title)\n",
    "    # plt.xlabel(\"x‚ÇÅ\")\n",
    "    # plt.ylabel(\"x‚ÇÇ\")\n",
    "    # plt.colorbar()\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train both models\n",
    "model_base, acc_base, losses_base = train_until_threshold(\n",
    "    FFnet, train_loader, test_loader,\n",
    "    max_retries=20, threshold=0.95,\n",
    "    depth=15, width=2, bottleneck=False, activation='relu'\n",
    ")\n",
    "\n",
    "model_bottleneck, acc_bottleneck, losses_bottleneck = train_until_threshold(\n",
    "    FFnet, train_loader, test_loader,\n",
    "    max_retries=10, threshold=0.95,\n",
    "    depth=15, width=2, bottleneck=True, activation='relu'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curve(losses_base, \"Loss - Baseline Model\")\n",
    "plot_loss_curve(losses_bottleneck, \"Loss - Bottleneck Model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot decision boundaries\n",
    "X_test, y_test = next(iter(test_loader))\n",
    "\n",
    "plot_decision_boundary(model_base, X_test.numpy(), y_test.numpy(), title=\"Baseline Model\")\n",
    "plot_decision_boundary(model_bottleneck, X_test.numpy(), y_test.numpy(), title=\"Bottleneck Model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mpl_toolkits.mplot3d import Axes3D  # needed for 3D plotting\n",
    "# from mpl_toolkits.mplot3d import axes3d\n",
    "# from matplotlib import cm  # for colormaps\n",
    "\n",
    "# def plot_decision_surface_3d(model, X, y, title=\"3D Prediction Surface\"):\n",
    "#     model.eval()\n",
    "#     x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "#     y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "#     xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "#                          np.linspace(y_min, y_max, 200))\n",
    "#     grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "#     grid_tensor = torch.tensor(grid, dtype=torch.float32)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         preds = model(grid_tensor).numpy().reshape(xx.shape)\n",
    "\n",
    "#     fig = plt.figure(figsize=(12, 9))\n",
    "#     ax = fig.add_subplot(111, projection='3d')\n",
    "#     surf = ax.plot_surface(xx, yy, preds, cmap=cm.coolwarm, edgecolor='none', alpha=0.9)\n",
    "\n",
    "#     # Plot original data points in 3D (z=prediction)\n",
    "#     with torch.no_grad():\n",
    "#         data_preds = model(torch.tensor(X, dtype=torch.float32)).numpy()\n",
    "\n",
    "#     ax.scatter(X[:, 0], X[:, 1], data_preds, c=y.squeeze(), cmap='bwr', edgecolor='k', s=30)\n",
    "\n",
    "#     ax.set_xlabel('X')\n",
    "#     ax.set_ylabel('Y')\n",
    "#     ax.set_zlabel('Prediction')\n",
    "#     ax.set_title(title)\n",
    "#     fig.colorbar(surf, ax=ax, shrink=0.5, aspect=10, label='Prediction Value')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # plot_decision_surface_3d(model_base, X_test.numpy(), y_test.numpy(), title=\"3D Decision Surface - Baseline Model\")\n",
    "    # plot_decision_surface_3d(model_bottleneck, X_test.numpy(), y_test.numpy(), title=\"3D Decision Surface - Bottleneck Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tunnels of blue class prediction values through the orange outer area is evidence of the embedding restrictions. Through more layers it is possible, that the tunnel gets narrower and narrower (further improving accuracy) but due to the embedding restictions, it cannot disappear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot SVs\n",
    "\n",
    "I am wondering if bottle necks improve training or if high accuracy correlates with SVs in some way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plots.plots\n",
    "from plots.plots import plot_singular_values_of_weightmatrix\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot activation heatmaps\n",
    "plot_singular_values_of_weightmatrix(model_base, title = 'Baseline model')\n",
    "plot_singular_values_of_weightmatrix(model_base, log_scale=False, title = 'Baseline model')\n",
    "plot_singular_values_of_weightmatrix(model_bottleneck, title = 'Bottleneck model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train wide model\n",
    "\n",
    "Models with augmented width have no embedding restrictions and we can observe that a singular value close to the origin is constructed almost immediatly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weight_heatmaps(model, title=''):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    linear_layers = [module for module in model.modules() if isinstance(module, nn.Linear)]\n",
    "\n",
    "    n_layers = len(linear_layers)\n",
    "    if n_layers == 0:\n",
    "        print(\"No Linear layers with parameters found.\")\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(1, n_layers, figsize=(3 * n_layers, 3), squeeze=False)\n",
    "    axes = axes[0]\n",
    "\n",
    "    for i, layer in enumerate(linear_layers):\n",
    "        weight = layer.weight.detach().cpu().numpy()\n",
    "        weight = abs(weight)\n",
    "        ax = axes[i]\n",
    "        im = ax.imshow(weight, cmap='viridis', vmin = 0, vmax = 5, aspect='equal')\n",
    "\n",
    "        ax.set_title(f\"Layer {i}\")\n",
    "        ax.set_xlabel(\"Out\")\n",
    "        ax.set_ylabel(\"In\")\n",
    "       \n",
    "\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.suptitle(\"Weight Matrices Heatmaps - \" + title)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wide, acc_wide, losses_wide = train_until_threshold(\n",
    "    FFnet, train_loader, test_loader,\n",
    "    max_retries=20, threshold=0.95,\n",
    "    depth=1, width=3, bottleneck=False, activation='relu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(model_wide, X_test.numpy(), y_test.numpy(), title=\"Wide Model\")\n",
    "plot_weight_heatmaps(model_wide, title=\"Wide Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap of the layer 1 shows that the inner shape is constructed by almost equally including the 3 different neurons of the 0th layer creating a smoothed out triangle shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a grid over the input space.\n",
    "grid_size = 100 # Adjust as needed.\n",
    "\n",
    "def psi_manual(x, func):\n",
    "    \"\"\"\n",
    "    x: a tensor of shape (2,) representing a point in R^2.\n",
    "    model: a function mapping R^2 to R^2.\n",
    "    \n",
    "    Returns:\n",
    "      The smallest singular value of the Jacobian of model at x.\n",
    "    \"\"\"\n",
    "    # Ensure x is a leaf variable with gradient tracking enabled.\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # Define a lambda function to ensure accurate input-output mapping\n",
    "    # func = lambda inp: model(inp, output_layer = False)\n",
    "    \n",
    "    # Compute the Jacobian using torch.autograd.functional.jacobian (compatible with Python 3.8)\n",
    "    jacobian = torch.autograd.functional.jacobian(func, x, create_graph=True)\n",
    "    \n",
    "    # Compute singular values using svdvals (available in PyTorch 1.8, compatible with Python 3.8)\n",
    "    singular_values = torch.svd(jacobian, compute_uv=False)[1]\n",
    "    \n",
    "   \n",
    "    return singular_values.detach().numpy()\n",
    "\n",
    "\n",
    "\n",
    "x_range = np.linspace(-2, 2, grid_size)\n",
    "y_range = np.linspace(-2, 2, grid_size)\n",
    "psi_values = np.zeros((grid_size, grid_size, 2))\n",
    "\n",
    "# Put the model in evaluation mode.\n",
    "model_wide.eval()\n",
    "func = lambda inp: model_wide(inp)\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate psi(x) over the grid.\n",
    "for i, xv in enumerate(x_range):\n",
    "    for j, yv in enumerate(y_range):\n",
    "        # Create a 2D point as a torch tensor.\n",
    "        x_point = torch.tensor([xv, yv], dtype=torch.float32)\n",
    "        psi_values[j, i,:] = psi_manual(x_point, func) #one subtlety here: if there is only one SV it gets broadcast to all dimensions of psi_values[j,i,:] in the last dimension. this reduces if statements for e.g. the last layer, but we need to notice that the SINGLE SV gets plotted twice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_singular_values_grid(model, grid_size=200):\n",
    "    x_range = np.linspace(-1, 1, grid_size)\n",
    "    y_range = np.linspace(-1, 1, grid_size)\n",
    "    psi_values = np.zeros((grid_size, grid_size, 2))\n",
    "\n",
    "    model.eval()\n",
    "    func = lambda inp: model(inp)\n",
    "\n",
    "    for i, xv in enumerate(x_range):\n",
    "        for j, yv in enumerate(y_range):\n",
    "            x_point = torch.tensor([xv, yv], dtype=torch.float32)\n",
    "            sv = psi_manual(x_point, func)\n",
    "\n",
    "            # If only one singular value is returned (e.g., at the final layer), broadcast it\n",
    "            if sv.shape[0] == 1:\n",
    "                psi_values[j, i, :] = sv[0]\n",
    "            else:\n",
    "                psi_values[j, i, :] = sv\n",
    "    return x_range, y_range, psi_values\n",
    "\n",
    "\n",
    "def plot_singular_values(x_range, y_range, psi_values, title=\"Singular Values\"):\n",
    "    min_sv = psi_values[:, :, 0]\n",
    "    max_sv = psi_values[:, :, 1]\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    # Plot min singular value\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.contourf(x_range, y_range, min_sv, levels=50, cmap='viridis')\n",
    "    plt.colorbar(label='Min Singular Value')\n",
    "    plt.title(f'{title} (Min)')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "\n",
    "    # Plot max singular value\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.contourf(x_range, y_range, max_sv, levels=50, cmap='viridis')\n",
    "    plt.colorbar(label='Max Singular Value')\n",
    "    plt.title(f'{title} (Max)')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "models = [\n",
    "    (model_base, \"Baseline Model\"),\n",
    "    (model_bottleneck, \"Bottleneck Model\"),\n",
    "    (model_wide, \"Wide Model\")\n",
    "]\n",
    "\n",
    "for model, name in models:\n",
    "    x_range, y_range, psi_vals = compute_singular_values_grid(model, grid_size=100)\n",
    "    plot_singular_values(x_range, y_range, psi_vals, title=name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SV plots show the very different structure of the NNs with 2 neurons per layer and the one with 3 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = FFnet(depth=15, width=2, bottleneck=True, activation='relu')\n",
    "epoch_step = 10\n",
    "losses_combined = []\n",
    "\n",
    "for plot in range(1,10):\n",
    "    _, _, losses_running = train_model(model_test, train_loader, test_loader,\n",
    "                epochs=epoch_step, lr=0.01, patience=300, batch_size=128)\n",
    "    plot_decision_boundary(model_test, X_test.numpy(), y_test.numpy(), title=\"Bottleneck Model at \" + str(epoch_step*plot))\n",
    "    plot_singular_values_of_weightmatrix(model_test, title = 'Bottleneck model at ' + str(50*plot))\n",
    "    for loss in losses_running:\n",
    "        losses_combined.append(loss)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses_combined)\n",
    "plot_loss_curve(losses_combined, \"Loss - Baseline Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = FFnet(depth=15, width=2, bottleneck=False, activation='relu')\n",
    "epoch_step = 10\n",
    "losses_combined = []\n",
    "\n",
    "for plot in range(1,10):\n",
    "    _, _, losses_running = train_model(model_test, train_loader, test_loader,\n",
    "                epochs=epoch_step, lr=0.01, patience=300, batch_size=128)\n",
    "    plot_decision_boundary(model_test, X_test.numpy(), y_test.numpy(), title=\"Base Model at \" + str(epoch_step*plot))\n",
    "    plot_singular_values_of_weightmatrix(model_test, log_scale=False, title = 'Base model at ' + str(epoch_step*plot))\n",
    "    for loss in losses_running:\n",
    "        losses_combined.append(loss)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_relu = FFnet(depth=15, width=2, bottleneck=False, activation='relu')\n",
    "epoch_step = 50\n",
    "losses_combined = []\n",
    "\n",
    "for plot in range(1,10):\n",
    "    _, _, losses_running = train_model(model_relu, train_loader, test_loader,\n",
    "                epochs=epoch_step, lr=0.01, patience=300, batch_size=128)\n",
    "    plot_decision_boundary(model_relu, X_test.numpy(), y_test.numpy(), title=\"Base Model at \" + str(epoch_step*plot))\n",
    "    plot_singular_values_of_weightmatrix(model_relu, log_scale=False, title = 'Base model at ' + str(epoch_step*plot))\n",
    "    for loss in losses_running:\n",
    "        losses_combined.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralODE",
   "language": "python",
   "name": "neuralode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
