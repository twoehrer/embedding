{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # ResNet Bias Manipulation Experiment\n",
    "\n",
    "\n",
    "\n",
    " This notebook trains a ResNet on a binary classification task (concentric circles).\n",
    "\n",
    " It performs the following specific steps:\n",
    "\n",
    " 1. Trains the model for 100 epochs.\n",
    "\n",
    " 2. Manually interrupts to set a specific bias value in a hidden layer.\n",
    "\n",
    " 3. Continues training for another 100 epochs to observe recovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.datasets import make_circles\n",
    "from matplotlib.colors import LinearSegmentedColormap, to_rgb\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# Configuration for High-Res plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Device config\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Seed setup for reproducibility\n",
    "# seed = 107\n",
    "seed = np.random.randint(1,200)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "print(f\"Global Seed: {seed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Model Definition (ResNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagonalLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, fixed_w=None):\n",
    "        super().__init__()\n",
    "        self.dim = min(in_features, out_features)\n",
    "        self.out_features = out_features\n",
    "        if fixed_w is None:\n",
    "            self.weight = nn.Parameter(torch.ones(self.dim))\n",
    "            self.fixed = None\n",
    "        else:\n",
    "            fixed_w = torch.as_tensor(fixed_w, dtype=torch.float32)\n",
    "            self.k_fixed = min(len(fixed_w), self.dim)\n",
    "            self.register_buffer(\"fixed\", fixed_w[:self.k_fixed])\n",
    "            n_rest = max(self.dim - self.k_fixed, 0)\n",
    "            self.weight_rest = nn.Parameter(torch.ones(n_rest))\n",
    "            self.weight = None\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "\n",
    "    def _weight_vec(self, x):\n",
    "        if self.fixed is None: return self.weight\n",
    "        return torch.cat((self.fixed.to(x.device, x.dtype), self.weight_rest), dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = self._weight_vec(x)\n",
    "        out = x[..., :self.dim] * w\n",
    "        pad = self.out_features - out.shape[-1]\n",
    "        if pad > 0: out = F.pad(out, (0, pad))\n",
    "        return out + self.bias\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, features, skip_param=1, sara_param=1, activation='relu', batchnorm=True, gain=1.0, bias=True):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc = nn.Linear(features, features, bias=bias)\n",
    "        nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
    "        if self.fc.bias is not None: nn.init.zeros_(self.fc.bias)\n",
    "        \n",
    "        if batchnorm: self.bn = nn.BatchNorm1d(features)\n",
    "        \n",
    "        if activation == 'relu': self.activation = nn.ReLU()\n",
    "        elif activation == 'tanh': self.activation = nn.Tanh()\n",
    "        elif activation == 'id': self.activation = nn.Identity()\n",
    "        \n",
    "        self.skip_param = skip_param\n",
    "        self.sara_param = sara_param\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.fc(x)\n",
    "        if hasattr(self, 'bn'): out = self.bn(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.sara_param * out + self.skip_param * identity\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_hidden, skip_param=1, sara_param=1, \n",
    "                 activation='relu', final_sigmoid=True, batchnorm=True, input_layer=True, \n",
    "                 input_layer_diagonal=False, fixed_w=None, bias=True):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_hidden = num_hidden\n",
    "        self.input_layer_exists = input_layer\n",
    "        self.activation_name = activation # Store for saving\n",
    "        \n",
    "        if activation == 'relu': self.activation = nn.ReLU()\n",
    "        elif activation == 'tanh': self.activation = nn.Tanh()\n",
    "        elif activation == 'id': self.activation = nn.Identity()\n",
    "        \n",
    "        if self.input_layer_exists:\n",
    "            if input_layer_diagonal:\n",
    "                self.input_fc = DiagonalLinear(self.input_dim, self.hidden_dim, fixed_w=fixed_w)\n",
    "            else:\n",
    "                self.input_fc = nn.Linear(input_dim, hidden_dim, bias=bias)\n",
    "            self.input_layer = nn.Sequential(self.input_fc, self.activation)\n",
    "\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(hidden_dim, skip_param=skip_param, sara_param=sara_param, \n",
    "                            activation=activation, batchnorm=batchnorm, bias=bias) \n",
    "              for _ in range(num_hidden)]\n",
    "        )\n",
    "        \n",
    "        # Final Layer\n",
    "        if final_sigmoid:\n",
    "            self.output_fc = nn.Sequential(nn.Linear(hidden_dim, output_dim, bias=bias), nn.Sigmoid())\n",
    "        else:\n",
    "            self.output_fc = nn.Linear(hidden_dim, output_dim, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.input_layer_exists: x = self.input_layer(x)\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.output_fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Training Logic\n",
    "\n",
    " (Using the provided training script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_true, type='class'):\n",
    "    if type == 'class':\n",
    "        y_pred_binary = (y_pred >= 0.5).int()\n",
    "        y_true_binary = y_true.int()\n",
    "        correct = (y_pred_binary == y_true_binary).sum().item()\n",
    "        total = y_true.shape[0]\n",
    "        return correct / total\n",
    "    if type == 'reg':\n",
    "        mse = torch.mean((y_pred - y_true) ** 2)\n",
    "        acc = 1.0 - mse\n",
    "        acc = torch.clamp(acc, min=0.0, max=1.0)\n",
    "        return acc.item()\n",
    "\n",
    "def train_model(model, train_loader, test_loader,\n",
    "                load_file=None, epochs=300, lr=0.01, early_stopping=True, patience=300, cross_entropy=True, seed=None):\n",
    "    \n",
    "    if load_file is None:\n",
    "        model.to(device) # Ensure model is on device\n",
    "        model.train()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        # Note: If final_sigmoid=False in ResNet, use BCEWithLogitsLoss. \n",
    "        # If final_sigmoid=True, use BCELoss. \n",
    "        # The provided script uses BCELoss, so model output must be [0,1].\n",
    "        if cross_entropy:\n",
    "            criterion = nn.BCELoss() \n",
    "        else: \n",
    "            criterion = nn.MSELoss()\n",
    "\n",
    "        best_acc = 0\n",
    "        patience_counter = 0\n",
    "        losses = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device) # Move data to device\n",
    "                \n",
    "                y_pred = model(batch_X)\n",
    "                loss = criterion(y_pred, batch_y)\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            losses.append(epoch_loss / len(train_loader))\n",
    "            \n",
    "            # Evaluation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                acc_summed = 0.\n",
    "                counter = 0\n",
    "                for X_test, y_test in test_loader:\n",
    "                    X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "                    counter += 1\n",
    "                    test_preds = model(X_test)\n",
    "                    acc_summed += compute_accuracy(test_preds, y_test, type='reg' if not cross_entropy else 'class')\n",
    "                acc = acc_summed / counter\n",
    "            model.train()\n",
    "            \n",
    "            if early_stopping:\n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                    best_model_state = copy.deepcopy(model.state_dict())\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        print(f\"⏹️ Early stopping at epoch {epoch}, best acc: {best_acc:.3f}\")\n",
    "                        break\n",
    "                if patience_counter > 0:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "            else: \n",
    "                best_acc = acc \n",
    "\n",
    "        return model, best_acc, losses\n",
    "    \n",
    "    else:\n",
    "        # (Load logic omitted for brevity as we are training from scratch)\n",
    "        pass\n",
    "\n",
    "def plot_loss_curve(losses, title=\"Training Loss\", filename=None):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(losses, label=\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename + '.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Helper Functions (Plotting & Bias Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, title=\"Decision Boundary\", amount_levels=50, plotrange=[-2.5,2.5], file_name=None, footnote=None):\n",
    "    \"\"\"\n",
    "    Visualizes the decision boundary.\n",
    "    \"\"\"\n",
    "    colors = [to_rgb(\"C0\"), [1, 1, 1], to_rgb(\"C1\")]\n",
    "    cm = LinearSegmentedColormap.from_list(\"Custom\", colors, N=amount_levels)\n",
    "    \n",
    "    model.eval()\n",
    "    x_min, x_max = plotrange[0], plotrange[1]\n",
    "    y_min, y_max = plotrange[0], plotrange[1]\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    grid_tensor = torch.tensor(grid, dtype=torch.float32).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(grid_tensor).cpu().numpy().reshape(xx.shape)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    levels = np.linspace(0., 1., amount_levels).tolist()\n",
    "    contour = ax.contourf(xx, yy, preds, levels=levels, cmap=cm, alpha=0.8)\n",
    "    \n",
    "    # Move data to cpu for plotting\n",
    "    X_cpu = X.cpu()\n",
    "    y_cpu = y.cpu()\n",
    "    \n",
    "    ax.scatter(X_cpu[:, 0], X_cpu[:, 1], s=25, c=y_cpu.squeeze(), cmap=cm, edgecolors='black', linewidths=0.5, alpha=0.9)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    \n",
    "    colorbar_ticks = np.linspace(0, 1, 9)\n",
    "    cb = plt.colorbar(contour, ax=ax, label='Prediction Probability', ticks=colorbar_ticks)\n",
    "    cb.set_ticklabels([f\"{tick:.2f}\" for tick in colorbar_ticks])\n",
    "    \n",
    "    if footnote:\n",
    "        plt.figtext(0.5, 0, footnote, ha=\"center\", fontsize=8)\n",
    "        \n",
    "    if file_name:\n",
    "        plt.savefig(file_name + '.png', bbox_inches='tight', dpi=300, facecolor='white')\n",
    "    plt.show()\n",
    "\n",
    "def set_layer_bias(model, layer_name, b_specific):\n",
    "    \"\"\"\n",
    "    Sets the bias of a specific layer to b_specific.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parts = layer_name.split('.')\n",
    "        module = model\n",
    "        for part in parts:\n",
    "            module = getattr(module, part)\n",
    "        \n",
    "        if hasattr(module, 'bias') and module.bias is not None:\n",
    "            with torch.no_grad():\n",
    "                module.bias.fill_(b_specific)\n",
    "            print(f\"✅ Set bias of '{layer_name}' to {b_specific}\")\n",
    "        else:\n",
    "            print(f\"⚠️ Layer '{layer_name}' has no bias parameter.\")\n",
    "    except AttributeError:\n",
    "        print(f\"❌ Could not find layer '{layer_name}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Experiment Setup: Data & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation\n",
    "n_points = 4000\n",
    "batch_size = 100\n",
    "X, y = make_circles(n_samples=n_points, noise=0.05, factor=0.5, random_state=seed)\n",
    "X = X * 2.0 # Scale\n",
    "\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "train_ds = TensorDataset(X_tensor, y_tensor)\n",
    "# For simplicity in this example, using same dataset for train/test to visualize overfitting/recovery clearly\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model Params\n",
    "num_hidden = 5 # number of hidden layers. The total network has additionl 2 layers: input to hidden and hidden to output\n",
    "input_dim = 2\n",
    "hidden_dim = 2\n",
    "output_dim = 1\n",
    "skip_param = 1\n",
    "sara_param = .2\n",
    "activation = 'tanh' #'relu' and 'tanh' are supported\n",
    "final_sigmoid = True\n",
    "batchnorm = False\n",
    "input_layer = False\n",
    "bias_setting = True #!!! THIS IS A CRAZY TRY TO SIMPLIFY\n",
    "\n",
    "model = ResNet(input_dim=2, hidden_dim=hidden_dim, output_dim=output_dim, \n",
    "               num_hidden=num_hidden, activation=activation, sara_param=sara_param,\n",
    "               final_sigmoid=final_sigmoid, bias=bias_setting, \n",
    "               batchnorm=False, input_layer=False).to(device)\n",
    "\n",
    "subfolder = 'resnet_bias_experiment'\n",
    "if not os.path.exists(subfolder): os.makedirs(subfolder)\n",
    "\n",
    "print(\"Model initialized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Stage 1: Train for 100 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Stage 1: Training (0 - 100 Epochs) ---\")\n",
    "model, acc1, losses1 = train_model(model, train_loader, test_loader, \n",
    "                                   epochs=100, lr=0.01, early_stopping=False, cross_entropy=True)\n",
    "\n",
    "print(f\"Stage 1 Accuracy: {acc1:.4f}\")\n",
    "\n",
    "# Visualize Stage 1\n",
    "X_plot, y_plot = next(iter(test_loader))\n",
    "plot_decision_boundary(model, X_plot, y_plot, \n",
    "                       title=f\"Boundary after 100 Epochs (Acc: {acc1:.2f})\", \n",
    "                       file_name=f\"{subfolder}/stage1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6. Bias Manipulation\n",
    "\n",
    " We will target the **3rd residual block** (`res_blocks.2.fc`) and set its bias to **5.0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer = 'res_blocks.2.fc'\n",
    "bias_value = 5.0\n",
    "\n",
    "print(f\"--- Interruption: Setting bias of {target_layer} to {bias_value} ---\")\n",
    "set_layer_bias(model, target_layer, bias_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weightmatrix(model, title='', ax=None):\n",
    "    \"\"\"\n",
    "    For each Linear layer in the model, compute the eigenvalues of the weight matrix,\n",
    "    take their modulus, and plot them by layer index (x-axis) vs. modulus (y-axis).\n",
    "    \"\"\"\n",
    "    from matplotlib.colors import TwoSlopeNorm\n",
    "    linear_layers = [module for module in model.modules() if isinstance(module, nn.Linear)]\n",
    "\n",
    "\n",
    "    for i, layer in enumerate(linear_layers):\n",
    "        W = layer.weight.detach().cpu().numpy()\n",
    "        # Normalize so that 0 is at the center (white)\n",
    "        \n",
    "        val_min, val_max = W.min(), W.max()\n",
    "    \n",
    "        if val_min < 0 and 0 < val_max:\n",
    "            # trivial case, no need for fancy normalization\n",
    "            norm = TwoSlopeNorm(vmin=val_min, vcenter=0, vmax=val_max)\n",
    "            plt.imshow(W, cmap=\"seismic\", norm=norm, origin=\"upper\")\n",
    "            \n",
    "\n",
    "        else:\n",
    "            print(val_max, val_min)\n",
    "            plt.imshow(W, cmap=\"seismic\", origin=\"upper\")\n",
    "            \n",
    "\n",
    "        plt.colorbar(label=\"Value\")\n",
    "\n",
    "        # Matrix indices\n",
    "        plt.xticks([0, 1], [\"1\", \"2\"])  # columns j\n",
    "        plt.yticks([0, 1], [\"1\", \"2\"])  # rows i\n",
    "        plt.xlabel(\"j\")\n",
    "        plt.ylabel(\"i\")\n",
    "        plt.title( str(i) + \"th layer\")\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "linear_layers = [module for module in model_res.modules() if isinstance(module, nn.Linear)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 7. Stage 2: Continue Training (100 Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Stage 2: Training (100 - 200 Epochs) ---\")\n",
    "# Note: train_model resets optimizer, so this simulates a 'restart' with the modified weights\n",
    "model, acc2, losses2 = train_model(model, train_loader, test_loader, \n",
    "                                   epochs=100, lr=0.01, early_stopping=False, cross_entropy=True)\n",
    "\n",
    "print(f\"Stage 2 Accuracy: {acc2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 8. Results & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine losses\n",
    "all_losses = losses1 + losses2\n",
    "\n",
    "# 1. Plot Loss Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(all_losses, label=\"Training Loss\", color='blue')\n",
    "plt.axvline(x=100, color='red', linestyle='--', label='Bias Injection (Epoch 100)')\n",
    "plt.title(\"ResNet Training Loss with Bias Perturbation\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(f\"{subfolder}/full_loss_curve.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 2. Plot Final Decision Boundary\n",
    "plot_decision_boundary(model, X_plot, y_plot, \n",
    "                       title=f\"Boundary after Bias Recovery (Acc: {acc2:.2f})\", \n",
    "                       footnote=f\"Epoch 200, Bias set to {bias_value} at Ep 100\",\n",
    "                       file_name=f\"{subfolder}/stage2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralODE",
   "language": "python",
   "name": "neuralode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
